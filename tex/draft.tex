\documentclass[12pt]{article}

\usepackage[draft]{fixme}
\fxsetup{layout=footnote}
\usepackage[left]{lineno}
\usepackage{amsmath,amsthm,amssymb,mathtools,graphicx,caption,subcaption,mathrsfs,cancel,multirow,enumitem,setspace,sectsty,cite,hyperref}
\usepackage{fullpage}

\sectionfont{\large}
\subsectionfont{\normalsize}

\input{commands}

\title{Naturalness and Long-lived Beyond the Standard Model Particles}
\author{Bryan Cardwell}

\begin{document}

\singlespacing

\maketitle

\begin{abstract}

I'll write an abstract here.

\end{abstract}

\newpage
\tableofcontents
\newpage
\doublespacing
\linenumbers

\section{The Standard Model}
    The standard model of particle physics (SM) describes all known particles and their non-gravitational interactions and is experimentally verified to astonishing \fxnote{weak word} accuracy. The SM consists of 12 spin-1/2 particles, the fermions, that make up all observed matter; 12 spin-1 particles, the gauge bosons, that communicate the electromagnetic, weak, and strong forces; and 1 fundamental scalar, the Higgs boson, which will be described in detail below.
    
    All SM particles transform according to the gauged symmetry group SU(3)cxSU(2)LxU(1)Y, where SU(3)c is associated with color charge, SU(2)L is associated with weak isospin, and U(1)Y is associated with weak hypercharge. The gauge bosons are associated with the generators of these groups, thus there are 8 gluons associated with SU(3)c, three W bosons associated with SU(2)L, and 1 B boson associated with U(1)Y. The fermions are catagorized by which representation of each group they fit into. For example, SU(3)c distinguishes the quarks (which live in the fundamental representation) from the leptons (which are singlets), and SU(2)L distinguishes the left-handed fermions (doublets) from the right-handed fermions (singlets). Finally, different values of weak hypercharge further distignuish SM particles according to a linear combination of their weak isospin and electric charge \fxnote{is there some more fundamental way to describe Y?}.

    The Higgs potential has a nonzero vacuum expectation value (VEV), which spontaneously breaks SU(2)LxU(1)Y to U(1)EM while leaving SU(3)c unscathed. Once the dust settles, the B and 3 W bosons have been mixed to produce the W+, W-, Z bosons that mediate the weak force and the photon that mediates electromagnetism. 3 of the 4 generators are broken in the process, leaving 3 extra degrees of freedom that ultimately become the masses of the W+, W-, and Z bosons while the remaining symmetry is associated with electric charge. Furthermore, expanding the Higgs field about its VEV produces mass terms for the quarks and charged leptons, thus giving mass to all massive SM particles. \fxnote{probably show a little math}

    \fxnote{kinda funny wording throughout. Maybe it would be better to trade some words for math?}
    All SM particles interact with virtual particles through loop interactions. These interactions lead to corrections to SM parameters such as particle masses. It is common to write a given parameter as a linear combination of its 'bare' (uncorrected) value and its corrections. As the only fundamental scalar in the SM, the Higgs is uniquely affected by these corrections. The next section explores the concept of naturalness, a major motivating principle for beyond the standard model (BSM) physics that is directly tied to the relationship between the Higgs bare mass and the terms that correct it.

\section{Naturalness and the Higgs mass}
    \fxnote{cite guidice naturally and t'hooft naturalness}
    \fxnote{Haven't specifically said 'hierarchy problem. Is that bad?}
    \fxnote{Never really talked about the effective theory concept or dimensional analysis}
    The naturalness criterion can be stated in many ways, a few of which are reproduced here: (1) All dimensionless parameters should be of order 1, (2) An effective theory should not be sensitive to small variations in the parameters of higher-energy theory, and (3) any dimensionless parameter much smaller than 1 must be protected by a custodial symmetry. When applied to the Higgs mass, naturalness leads to what's known as the 'hierarchy problem' or unacceptable levels of 'fine tuning'. These issues are unpacked below.

    Experimentally, the mass of the Higgs boson is 125 GeV, a value that must be achieved by a linear combination of the bare mass and loop diagram corrections. The scale of the loop corrections is proportional to the square of the cutoff scale of the SM, which is the highest energy at which the SM is valid. If, hypothetically, the SM is valid all the way up to the Planck scale (where quantum gravity becomes important), then Higgs mass corrections would be order($MP^2$) $\approx 10^{32}$ GeV.
    
    Order($10^{32}$) corrections imply that the bare Higgs mass must be negative and also order($10^{32}$) to reproduce the experimetnally known value. In fact, the bare mass and corrections must match to 1 part in $\approx 10^{30}$. While it's entirely possible that these two numbers just so happen to match so perfectly, many \fxnote{weak} feel that such miraculous fine tuning must be a hint that some deeper physical mechanism is at work. That is, either some previously unknown symmetry is protecting the Higgs mass or the cutoff scale is order(TeV).

    Before digging into possible solutions to this problem\fxnote{specify}, one other question must be addressed: why doesn't this same argument apply to all SM particles? The answer comes from naturalness statement (3) above. As a fundamental scalar, the Higgs boson lacks the chiral or gauge symmetries enjoyed by the fermions and vector bosons, respectively. These two symmetries cause the symmetry of the SM to be enhanced when the fermion and vector boson masses are set to 0. \fxnote{work out fermion case?} Setting the higgs mass to 0 in no way increases the symmetry of the theory, so we have no reason to expect it to be small.

    There are three main approaches to resolving this problem\fxnote{specify}: (1) argue that it was never really a problem in the first place, (2) reduce the SM cutoff scale, and (3) introduce a new symmetry to protect the Higgs mass. Arguments against naturalness often involve the anthropic principle, which claims that it should come as no surprise that humans live in a universe whose fundamental parameters are tuned to produce carbon-based life. Alternatively, some theories, such as large extra dimensions, lower the SM cutoff by proposing that gravity is spread across more than 3 spatial dimensions, which lowers the Planck mass itself. Finally, the dominant BSM theory that aims to solve the naturalness problem by introducing a new symmetry is supersymmetry, which is explained below. \fxnote{didn't mention composite higgs}

    \fxnote{Optional paragraph about non-Higgs unnaturalness:    There are other unnatural quantities that people worry about in physics. I know of two off the top of my head: (1) thetaQCD in the strong CP problem (2) the cosmological consant. The strong CP problem comes from the fact that QCD technically allows CP violation even though such violation has never been observed. The degree of violation is parameterized as thetaQCD, which recasts the problem into the naturalness context of 'why is thetaQCD so small?'. The cosmological constant is experimentally $\approx$60 to $\approx$120 orders of magnitude smaller than the predicted value \fxnote{predicted how?}. While these issues deserve serious attention, they will not be considered further here}
    
\section{Supersymmetry}
    Supersymmetry (SUSY) introduces a new symmetry in which every SM field fits into a larger multiplet with an inherent symmetry between bosons and fermions. In its simplest form, SUSY results in one new boson for every SM fermion and one new fermion for every SM boson \fxnote{and an extra Higgs field - can I gloss over that?}. The increase in particle multiplicity calls for a new naming convention: the spin-0 superpartners to the SM fermions are referred to as sfermions (e.g. stop or smuon), and the spin-1/2 superpartners to the SM bosons are given the name of their SM counterpart with "ino" tacked on to the end (e.g. Higgsino or wino). Together the superpartners to the SM particles are known as sparticles. 

    When calculating contributions from loop diagrams, one finds that fermion loops differ in sign from boson loops, which means that in SUSY every bosonic correction to the Higgs mass is cancelled by a fermionic correction and vice versa. In fact, exact supersymmetry predicts that the bare Higgs mass is uncorrected to all orders in perturbation theory. \fxnote{show example diagram with contribution?}

    But the lack of sparticles in collider searches and everyday life tells us that SUSY must be broken. If SUSY were exact, every superpartner would have the exact same mass as its SM counterpart, which means all sparticles would have been found at the same energy scales as SM particles. The mechanism and scale of SUSY breaking is an open question with many competing answers that achieve different theoretical goals and lead to different TeV-scale phenemena. A handful of models with particular approaches to SUSY breaking that are particularly prominant or particularly relevant to naturalness or long-lived particles are gauge-mediated susy breaking (GMSB), anomaly-mediated SUSY breaking (AMSB), Split SUSY, and R-parity violating SUSY (RPV). The details of these models will be explored as needed throughout this paper.

\section{Long-lived Particles}
    The SUSY variants most relevant to this discussion are those that are motivated by naturalness and \fxnote{and/or?} produce a signal that could be missed by common SUSY search techniques. Specifically, we are interested in SUSY models that predict new long-lived particles (LLPs). \fxnote{incomplete sentence} That is, particles whose lifetimes are such that they decay at least a detectable distance from the nominal interaction point. This category includes everything from particles that decay at \fxnote{give ballparky number ($\approx$100um?)} to particles that propagate through the entire detector. 

    In the SM and in BSM theories, LLPs result from one of the following conditions: (1) a symmetry that guarantees stability, (2) weak couplings to decay products, (3) highly-virtual intermediate states, or (4) limited phase space in which to decay. In the SM, charge and baryon number conservation guarantee the absolute stability of the electron and proton as the the lightest particles carrying their respective conserved quantities. Neutrons and muons, on the other hand, are long-lived beause the proton and electron masses leave them little phase space in which to decay \fxnote{also mention that they're both weak decays?}. \fxnote{Mention neutrino, pion, kaon, some other bound state I might be forgetting?}

    In SUSY models, LLPs arise for the same basic reasons, and different SUSY models meet different LLP criteria in different ways. The most common \fxnote{based on my assumptions} SUSY LLP occurs in models with conserved R-partity, which is a proposed symmetry that protects many models from proton decay. In models with R-parity, the SM particles are assigned R-parity +1 and their superpartners are assigned R-parity -1. Conserving the product of R-parity at each vertex has two immediate phenomenalogical consequences: (1) sparticles can only be produced in pairs and (2) the lightest supersymmetric particle (LSP) is absolutely stable. A neutral LSP could show up in collider searches as significant missing energy, making MET the conventional SUSY signature in many searches.

    On the other hand, models with R-parity violation (RPV) produce long-lived, but not perfectly stable LSPs when RPV terms have small coupling constants. Luckily, RPV models can still avoid proton decay by allowing either terms that violate baryon number or terms that violate lepton number, but not both. A similar situation arises in GMSB models where the gravitino is the LSP. The coupling between the next-to-LSP (NLSP) and gravitino is inversely proportional to the scale of SUSY breaking, which supresses the NSLP decay rate and can lead to LLPs. \fxnote{is now a good time to explain GMSB?}\fxnote{cite dimopoulos low energy}

    LLPs from highly-virtual intermediate particles can occur in models such as split SUSY, where the scalar sparticles are significantly more massive than the gauginos. In these models, the gluino becomes long-lived when its decay to a quark and neutralino is mediated by a highly-virtual squark. \fxnote{cite kilian} \fxnote{mention that split throws out naturalness motivation?}

    The degenerate mass spectra of other SUSY models can also give rise to LLPs. In some AMSB models \fxnote{explain AMSB?}, for example, the zino and wino are the LSP and NLSP, respectively, and are nearly degenerate in mass. The wino then becomes long-lived when its decay is suppressed by the lack of available phase space. \fxnote{cite R\&S out of this world}
    \fxnote{merge last 2 paragraphs under 'mass spectra lead to LLP' concept?}
    \fxnote{never said anything about hidden valley or dark sector stuff. Never said the word R-hadron. Never said stealth or displaced SUSY. Should I?}

\section{The CMS Detector}
\subsection{Overview}
    \fxnote{Add LHC description}  
    \fxnote{cite cms experiment at the cern lhc}
    The Compact Muon Solenoid (CMS) detector is designed to reconstruct a variety of particles from 14 TeV proton-proton collisions at the Large Hadron Collider (LHC). To meet this goal, CMS combines information from several subdetectors nested radially in and around a 4-T superconducting solenoid. All together, CMS is 21.6 m long, 14.6 m across, and weighs approximately 12500 tons. The subdetectors that make up CMS are described below.

\subsection{Tracker}
    \fxnote{cite CMS TDR, jinst 12 c02033}
    \fxnote{maybe I should trade some number listing for talking about how silicon detectors work}
    \fxnote{what about readout, cooling, etc?}
    In the region closest to the interaction point, CMS uses a fast, high-granularity silicon tracker to reconstruct tracks and pinpoint primary and secondary vertices. The tracker is subdivided into two regions: at r $<$ 20 cm, the high particle flux \fxnote{specify} requires small pixels to keep occupancy near 0.1\%. Moving farther from the IP decreases particle flux, which allows the use of larger silicon strips.

    The pixel tracker, which was upgraded in winter 2016/17, makes up the innermost layer and features approximately 124 million silicon pixels, each of which covers an active area of 100 by 150 $um^2$. The pixels are spread between four barrel layers (BPIX) at 2.9 $<$ r $<$ 16 cm and 3 endcap rings (FPIX) at 29 $<$ z $<$ 52 cm. The upgraded pixel tracker should provide 4-hit tracking out to eta $<$ 2.4 and improved b-tagging over the original pixel tracker despite the increased pileup expected in 2017.
    
    Just outside the pixel tracker, the silicon strip tracker (SST) occupies the 20 $<$ r $<$ 116 cm and $|z|$ $<$ 282 cm region of CMS. Composed of 9.3 million silicon strips that vary from \_ to \_ in size \fxnote{look up} depending on particle flux in the region, the SST covers an active area of roughly 196 $m^2$. At each trigger, the strip output voltage is transferred via optical link to be read out by off-detector front-end electronics \fxnote{right jargon?}.

\subsection{Electromagnetic Calorimeter}
    \fxnote{cite CMS TDR}
    \fxnote{talk about readout?}
    \fxnote{More in the way of concept/how stuff works?}
    After traversing the inner tracker, a particle would next encounter the electromagnetic calorimeter (ECAL). As a homogeneous scintillation calorimeter, ECAL uses 61200 lead tunstate crystals in the barrel and 7324 in each endcap to reconstruct the energy deposited during electromagnetic showers. Lead tungstate crystals allow for a fast (80\% of light within 25 ns), compact (radiation length = 0.89 cm), fine-grained (Moliere length = 2.2 cm), and radiation hard (up to 10 Mrad\fxnote{learn context for this number}) calorimeter. The main drawback is the relatively low light yield (30 photon / MeV), which necesitates photodetectors with intrinsic gain that work in magnetic fields \fxnote{say B-field part better}. 

    The barrel section (EB) extends radially from 129 to 177 \fxnote{look up} cm and covers a pseudorapidity range of 0 to 1.479. The crystals are tapered to approximately project back to the IP but not so perfectly that likely particle trajectories align with cracks. Each crystal is approximately one Moliere radius wide and 25.8 radiation lengths deep.

    The crystals in each endcap section (EE) are arranged in an x-y grid that starts at $|z|$ = 315 cm and covers a pseudorapidity range of 1.479 to 3.0. Additionally, a preshower detector, which consists of two planes of silicon strip detectors and a lead absorber, sits just inside the grid of lead tungstate crystals. \fxnote{Say what the preshower is for? Is it for pi0 rejection? If so, how and why?}

\subsection{Hadronic Calorimeter}
    \fxnote{cite CMS TDR}
    \fxnote{talk about readout?}
    The hadronic calorimeter (HCAL) uses 3.7mm plates of plastic scintillator interspersed within brass absorber to reconstruct the energy deposited during hadronic showers. Embedded wavelength-shifting fibers capture the scintillation light and transfer it to clear fibers so it can eventually be read out by hybrid photodiodes.

    The barrel section (HB) spans a pseudorapidty range of -1.4 to 1.4 and has 2304 towers \fxnote{check}, each of which contains 14 \fxnote{check} brass plates and covers an area of 0.087 x 0.087 in eta-phi \fxnote{get a feel for this area}. In addition, an extra layer (or two at eta = 0) of scintillator tiles sits just outside the solenoid. This extra layer, known as hadron outer (HO), spans a pseudorapidty range of -1.26 to 1.26 and increases the minimum effective HCAL interaction length to greater than 11.8.

    Each endcap spans a pseudorapidity range of 1.3 to 3.0 with 14 towers in eta and 5 to 10 degree phi segmentation. Also, a steel and quartz fiber forward calorimeter (HF) sits 11.2 m from the interaction pipe in a pseudorapidty range of 3.0 to 5.0. In HF, particles produce Cherenkov light when traversing the quartz fibers that run parallel to the beamline. \fxnote{say why HF is useful or mention crazy flux?}

\subsection{Solenoid}
    \fxnote{cite cms experiment at the cern lhc}
    CMS employs a 4 T superconducting solenoid to aid in the measurement of charged particle momenta. The solenoid consists of 2168 turns of Nb-Ti superconducting cable and has an inner diameter of 6 m and a length of 12.5 m. The flux returns through an iron yoke that also houses the muon system. The significant \fxnote{quantify?} bending power helps CMS meet its physics goals of unambiguous sign determination and momentum resolution of 10\% for muons up to 1 TeV.

\subsection{Muon System}
    \fxnote{cite CMS TDR}
    \fxnote{Specifically mention CMS design specs to meet physics goals?}
    The muon system (MS), which is the outermost subdetector, detects muons with 3 varieties of gaseous ionization detectors: drift tubes (DTs) in the barrel, cathode strip chambers (CSCs) in the endcaps, and resistive plate chambers (RPCs) in both the barrel and endcaps. The choice of detector varies between barrel and endcap due to the difference in neutron-induced background, muon rates, and magnetic field strength. The RPCs are useful in the endcaps and barrel because their fast response and good time resolution allow unambiguous bunch crossing identification. Combining MS and tracker measurements improves the muon pT resolution at high momenta where the tracker pT resolution declines \fxnote{quantify or show std plot?}.

    In the barrel detector (BD), four layers of DTs and and RPCs are embedded in the solenoid return yoke at 4.0, 4.9, 5.9, and 7.9 m. \fxnote{Say how DT works?} The maximum drift length is 2 cm and the single point resolution is 200 um. When reconstructing a vector describing the muon momentum, the phi precision is better than 100 um in position and 1 mrad in direction. 

    Each muon endcap detector (ME) is composed of 234 CSCs and 4 RPC stations \fxnote{figure out exactly what RPC station means and say that instead}. Each CSC has six gas gaps with radial cathode strips and nearly perpendicualar planes of anode wires. The wire signal from the ionization-induced electron avalanch is fast enough to use in the L1 trigger, while the slower \fxnote{quantify} cathode strip signal provides precise \fxnote{quantify} position measurements. All in all, the CSC spacial resolution is approximately 200 um and the angular resolution is approximately 10 mrad.

\subsection{Trigger}
    \fxnote{cite CMS TDR, jinst 12 c03021}
    \fxnote{Is entire detector read out at L1?}
    \fxnote{Mention fpgas in L1 or more details about HLT?}
    \fxnote{Something about particle flow?}
    \fxnote{Sloppy, but I'm getting tired}
    The trigger reduces the data writing rate from the 40 MHz collision rate to less than 1 kHz so that events can be written to tape. The rate reduction happens in two stages: Level-1 (L1) and High-Level Trigger (HLT). L1 uses input from ECAL, HCAL, and MS that represents physics objects (e/gamma, tau, jets, MET, and mu) and uses custom electronics to reduce the rate to $\approx$100 kHz in 3.8 us. The HLT then uses a dedicated processor farm to further reduce the rate to the desired $<$ 1 kHz.

\section{LHC Searches for Long-lived Particles}
\subsection{Overview}
    LLPs leave distinct signatures that differentiate LLP signals from SM backgrounds but also add complexity to LLP analyses. CMS and ATLAS were generally designed to detect and reconstruct stable particles from prompt decays, so LLP analyses often need to stray from the traditional analysis path in terms of object reconstruction \fxnote{really reconstruction? Maybe object definition?}, background estimation, and which data tier to use \fxnote{can I say this more generally?}. This paper will focus on four complementary LLP signatures: displaced vertices, disappearing tracks, slow-moving particles, and particles that stop completely before decaying.

    Displaced vertices and disappearing tracks both look for particles that are unstable on detector timescales \fxnote{quantify?}. Displaced vertices searches are sensitive to LLPs that decay to visible particles in the inner volume \fxnote{specify} of the detector and leave tracks that can be traced back to a vertex that is sufficiently displaced from the nominal interaction point \fxnote{wordy}. Disappearing track searches are sensitive to LLPs that decay to particles that are either neutral or too soft to be reconstructed. Together, these two types of analyses cover \fxnote{something something} parameter space of LLPs that decay on detector timescales.

    Slow and stopped particle searches are both sensitive to LLPs that are detector-stable but far more massive than any stable SM particle. Searches for low-velocity particles use ionization energy loss per unit distance (dE/dX) and time-of-flight (TOF) measurements to look for particles that are moving more slowly than the expected SM particles \fxnote{redundant}, which normally \fxnote{quantify} have beta $>$ 0.9. Stopped particle searches take this idea one step further and look for particles that actually come to rest inside the detector before decaying, a phenomenon that could be possible with long lifetimes and small initial kinetic energy. These analyses look for calorimeter energy deposits or hits in the muon system that are out of time with proton bunch crossings that could be due to the decay of stopped particles.

    These four approaches cover a wide range of LLP lifetimes that would be missed in traditional searches. The next section will look  at a recent example of each analysis type from either ATLAS or CMS.

\subsection{Displaced Vertices}
    \fxnote{cite CONF-2017-026}
    LLPs that decay before reaching the inner tracker but still live longer than the vast majority of SM particles can be identified with a displaced vertex (DV). That is, a reconstructed vertex far from the nominal interaction point. ATLAS performs a search for such particles by looking for at least one DV along with large MET and large track multiplicity. The search is sensitive to LLPs with lifetimes in the 20 ps to 10 ns range.

    This search uses a simplified model in the split SUSY paradigm as a benchmark. In the simplified model, gluinos are kinematically accessible while squarks are not. The relevant process, then, is pair produced gluinos that decay to a quark and virtual squark, which then decays to a quark and the LSP, which is presumed to be a neutralino \fxnote{diagram?}. Because the squark mass is so high, the gluino lives long enough to form an R-hadron \fxnote{first use. Define here if not in LLP section} with SM quarks and propogate some measureable distance away from the IP before decaying. 

    Jets and MET are reconstructed as in standard analyses, but special care must be taken when looking for displaced tracks. The standard ATLAS tracking algorithm constrains the transverse and longitudinal impact parameters of potential tracks, so the authors employ an additional algorithm, referred to as large-radius tracking (LRT). LRT is run after the standard tracking algorithm and uses only hits that were not used in standard track reconstruction. By relaxing d0, z0, and number of hits requirements, LRT is able to reconstruct tracks that point back to DVs.
    
    Because no SM process will produce events that meet the mass and track multiplicity requirements on the DV, only instrumental backgrounds are considered. Specifically, the authors consider hadronic interactions in detector material, incorrectly combined vertices from short-lived SM particles, and incorrectly added hits from unrelated tracks to low-mass vertices.

    The authors minimize the effect of hadronic interactions by vetoing the material-dense regions of the detector and estimate the residual background by extrapolating from the low-mass region, which is dominated by hadronic interactions. After a dedicated study, the background from merged vertices was found to be negligable. Finally, the background from crossing tracks is determined with a model that adds pseudo-tracks to data vertices and is normalized by comparing the sample of 3-track vertices in data to (2+1)-track vertices in the model.

    After estimating 0.02 +- 0.02 background events, exactly 0 are observed. The result is then interpreted as an upper limit on the gluino mass and pair-production cross section as a function of gluino lifetime. For an LSP neutralino mass of 100 GeV, the gluino mass limit is approximately 2000 GeV for lifetimes in the 20 ps to 10 ns range. \fxnote{Include plots}

\subsection{Disappearing Tracks}
    \fxnote{cite CONF-2017-017}
    \fxnote{nice disappearing track cartoon}
    \fxnote{nice cartoon feynman diagrams of signal processes}
    Certain models with nearly mass-degenerate LSP and NLSP have the potential to present a remarkable signal: charged particles that seem to disappear in the tracker. Specifically, models with winos as the lightest gauginos, such as many AMSB models, will have a chargino NLSP with an observable lifetime (0.2 ns is a typical estimate) that eventaully decays to the neutralino LSP and, commonly, a pion. The nearly-degenerate masses have two important phenomenological roles: (1) increase the chargino lifetime, and (2) ensure low-momentum decay products. The low-momentum pion is then unlikely to be constructed, which results in a disappearing track.

    Both ATLAS and CMS looked for disappearing tracks during Run I, but the tracker improvements installed in ATLAS before 2015 and CMS before 2017 should increase sensitivity to decays at shorter radii \fxnote{I haven't mentioned previous iterations when talking about other analyses}. The ALTAS search presented here is sensitive to LLP lifetimes of approximated 10 ps to 10 ns \fxnote{make sure I'm talking about lifetime sensitivity in the same way across analyses}.

    The authors consider two signal processes: (1) electroweak gaugino production resulting in a disappearing track, an ISR jet, and MET and (2) gluino pair production resulting in a disappearing track, four jets, and MET. Process (1) could be the only gaugino production mode at the LHC if the gluino mass is kinematically unaccessable, but process (2), which leads to chargino production in cascade decays, will be dominant if the gluino mass is within reach. The ISR jet in process (1) is not physically required; it simply provides something for the gluinos to recoil off of so the MET signal isn't lost \fxnote{My interpretation. Run it by someone who knows stuff}.

    Disappearing track candidates, called pixel tracklets, are selected with a looser track reconstruction algorithm that runs over hits that were not associated to tracks by the standard reconstruction algorithm. The pixel tracklets are required to pass several isolation, pT, and quality requirements such as number of pixel layer hits. Finally, the disappearing criteria is enforced by requiring exactly zero associated hits in the subsequent tracking layer. Event selection requires one pixel tracklet, no electron or muon candidates (to suppress ttbar W/Z+j processes), and the MET and jet requirements associated with either of the two production modes described above.
    
    Pixel tracklets can be faked by random combinations of unrealted hits from nearby particles or by pixel hits that don't have the proper hits from the next tracking layer associated to them, either due to a hadron suffering a hard scatter or a lepton radiating a hard photon \fxnote{lots of words}. These backgrounds come primarily from ttbar and W+jets processes, and estimates for each type of background are obtained from data. \fxnote{I initially wrote a whole big thing about how each bg was estimated, but I'm thinking it was overkill}

    No excess over expected background is observed, so upper limits are placed in the NLSP chargino mass-lifetime plane for the EW signal process and in the NLSP chargino mass gluon mass plane for the QCD production process \fxnote{hopefully will be way clearer after texing in sparticle symbols}. \fxnote{Either include plots or say numbers}

\subsection{Stopped Particles}
    \fxnote{Add calorimeter search?}  
    \fxnote{cite CMS PAS EXO-17-004}
    \fxnote{mention how stopped particles complement HSCP search?}
    \fxnote{too redundant with overview?}
    With long enough lifetimes and sufficient mass, LLPs could come to rest in the detector before decaying. The eventual decay would then lead to energy deposits in the calorimeters or hits in the muon system that could be out of time with proton bunch crossings. A recent CMS analysis looked for such a signal in dimuon final states using the full 2015 and 2016 datasets. 

    This analysis is interpreted in the context of two potential LLPs: gluinos and multiply charged massive particles (mchamps). Long-lived gluinos can appear in models such as Split SUSY, where the squarks and sleptons are quite heavy, but the gluino is relatively light. After being pair produced, gluinos could form R-hadrons and eventually decay to quark-antiquark pairs along with NLSP neutralinos, which then decay to LSP neutralinos along with muon-antimuon pairs. The gluino decay is supressed by the large squark mass, which leads to its appreciable lifetime.

    This analysis uses the muon system to look for pairs of muons that arrive out of time with bunch crossings. The main backgrounds to such a signal are cosmics, beam halo, and muon system noise. Beam halo and muon system noise are easily removed by the selection criteria, but distinguishing signal from cosmics requires a more nuanced approach. After generating signal MC and modeling cosmics from data obtained in dedicated cosmic runs, the authors found that cosmics and signal could be differentiated using time-of-flight information from the muon system RPCs and DTs. Essentially, the direction of muon momentum could be gleaned from the timing information, and the authors required outgoing muons in both the upper and lower hemispheres.
    
    In addition to the above criteria, the authors developed a new muon reconstruction algorithm, called displaced standalone (DSA), that uses only muon system hits and does not constrain the location of the primary vertex in any way. The full selection requires exactly one good DSA track in each hemisphere with pT greater than 50 GeV, no reconstructed vertices, and several criteria related to the quality of the reconstructed track and timing measurement. Finally, restrictions on the direction of muon momentum (incoming vs outgoing), relative time measurements, and maximum number of CSC hits (0) minimize background from beam halo and cosmics.

    After estimating the background by extrapolating data from the cosmic-enriched region to the signal region and determining systematic uncertainties related to the MC simulation, trigger acceptance, and luminosity, the authors calculated the expected background for LLP lifetimes ranging from 100 ns to 1 us. In all cases, the expected background was less then one event. After unblinding, the observed number of events is exactly zero in every lifetime scenerio. These results are then interpreted as upper limits on the signal production cross section \fxnote{include limit plots or table}, which are the first limits for stopped particles that decay to muons at the LHC.

\subsection{Heavy Stable Charged Particles}
    \fxnote{cite CMS EXO7-010}
    \fxnote{nice dE/dx plot (fig 1)}
    Particles that live for more than a couple ns \fxnote{Check this and be more exact} will appear stable in CMS and ATLAS because they can traverse the entire detector before decaying. If such a particle is also sufficiently massive, it can be catigorized as a heavy stable charged particle (HSCP). HSCPs are characterterized by a greater rate of energy loss via ionization due to the lower speed with which they pass through detector material. Additionally, HSCPs can be fractionally, singly, or multiply charged, which also affects their rate of energy loss. 

    HSCPs will often be misidentified or unobserved by standard reconstruction algorithms and analysis criteria, which tend to assume beta$\approx$1 and $|q|$ = e. Inverting these assumptions provides two handles for a dedicated HSCP search in the context of three benchmark models: (1) HSCPs that form R-hadrons that either change the sign of their electric charge or become neutral before reaching the MS, (2) lepton-like HSCPs (quasi-stable sleptons) that are produced either through decays of squarks or gluinos or directly if squarks and gluinos are kinematically inaccessible, and (3) long-lived leptonlike fermions with arbitrary electric charge. This discussion will focus on the first two models, as they bear more directly on naturalness and SUSY.

    \fxnote{I could talk about how low-beta tracks will show up as zig-zags in DTs b/c reconstruction assumes beta$\approx$1. Zig-zag offset is used to estimate beta. Not sure where to fit it in.} \fxnote{Worth mentioning tracker-only vs tracker+TOF}

    The trigger requires all events to have either a high-pT muon or large MET. The high-pT muon trigger is more efficient for all benchmark models except the case where the R-hadron becomes neutral before reaching the MS. In the case where the R-hadron does not leave a track in the MS, the large MET trigger should recover some events.  

    The authors estimate the signal-region background using an ABCD method in which events are binned in a 2D plane with uncorrelated axes and the ratio of events in two control regions is used to extrapolate into the signal region from a third control region \fxnote{Do I need to explain this? If I do, I should be more clear}. Systematic uncertainties are introduced in background estimation, signal acceptance, and integrated luminosity.

    No significant excess over predicted background is observed, so the results are interpreted as upper limits in the production cross section - mass plane for the considered models. By comparing the observed and theoretical limits, the authors also extract mass limits for the gluinos, stops, and staus in various models. \fxnote{Include plots - table is too big and too many numbers to list}

\section{Experimental Viability of Natural BSM}
\subsection{Limits Summary}
\subsection{What makes a SUSY model natural?}

\section{Conclusion and Important Searches for 2017}

\clearpage
\pagebreak
\singlespacing
\bibliography{candidacy}{}
\bibliographystyle{auto}

\end{document}
